% BELM - Bidirectional Extreme Learning Machine Class
%   Train and Predict a SLFN based on Bidirectional Extreme Learning Machine 
%
%   This code was implemented based on the following paper:
%
%   [1] Yang, Y., Wang, Y., & Yuan, X. (2012).
%       Bidirectional extreme learning machine for regression problem and its learning effectiveness. 
%       IEEE Transactions on Neural Networks and Learning Systems, 23(9), 1498â€“1505. 
%       https://doi.org/10.1109/TNNLS.2012.2202289
%
%   Attributes:
%       Attributes between *.* must be informed.
%       BELM objects must be created using name-value pair arguments (see the Usage Example).
%
%         *numberOfInputNeurons*:   Number of neurons in the input layer.
%                Accepted Values:   Any positive integer.
%
%          numberOfHiddenNeurons:   Initial number of neurons in the hidden layer (default = 0)
%                                   It can be modified in the constructor, but it will be ignored.
%                                   After training, this attribute contains the number of 
%                                   neurons in the hidden layer.
%
%       maxNumberOfHiddenNeurons:   Maximum number of neurons in the hidden layer
%                Accepted Values:   Any positive real number (default = 1000)
%
%                   maximumError:   Maximum error (used as stopping criterion)
%                Accepted Values:   Any positive real number. (default = 1e-3)
%           
%          numberOfNeuronsByStep:   Number of neurons added in each iteration
%                Accepted Values:   Any positive integer number. (default = 1)
%
%             activationFunction:   Activation funcion for hidden layer
%                Accepted Values:   Function handle (see [1]) or one of these strings:
%                                       'sig':     Sigmoid (default)
%                                       'sin':     Sine
%
%                           seed:   Seed to generate the pseudo-random values.
%                                   This attribute is for reproducible research.
%                Accepted Values:   RandStream object or a integer seed for RandStream.
%
%       Attributes generated by the code:
%
%                    inputWeight:   Weight matrix that connects the input
%                                   layer to the hidden layer
%
%            biasOfHiddenNeurons:   Bias of hidden units
%
%                   outputWeight:   Weight matrix that connects the hidden
%                                   layer to the output layer
%
%   Methods:
%
%           obj = BELM(varargin):   Creates IELM objects. varargin should be in
%                                   pairs. Look attributes
%
%           obj = obj.train(X,Y):   Method for training. X is the input of size N x n,
%                                   where N is (# of samples) and n is the (# of features).
%                                   Y is the output of size N x m, where m is (# of multiple outputs)
%
%          Yhat = obj.predict(X):   Predicts the output for X.
%
%   Usage Example:
%
%       load iris_dataset.mat
%       X    = irisInputs';
%       Y    = irisTargets';
%       [~,Y] = max(Y,[],2); % Only one dimensional targets are supported by this method
%       belm  = BELM('numberOfInputNeurons', 4, 'maxNumberOfHiddenNeurons',100);
%       belm  = belm.train(X, Y);
%       Yhat = belm.predict(X)
%
%   License:
%
%   Permission to use, copy, or modify this software and its documentation
%   for educational and research purposes only and without fee is here
%   granted, provided that this copyright notice and the original authors'
%   names appear on all copies and supporting documentation. This program
%   shall not be used, rewritten, or adapted as the basis of a commercial
%   software or hardware product without first obtaining permission of the
%   authors. The authors make no representations about the suitability of
%   this software for any purpose. It is provided "as is" without express
%   or implied warranty.
%
%       Federal University of Espirito Santo (UFES), Brazil
%       Computers and Neural Systems Lab. (LabCISNE)
%       Authors:    B. L. S. Silva, F. K. Inaba, D. L. Cosmo
%       email:      labcisne@gmail.com
%       website:    github.com/labcisne/ELMToolbox
%       date:       Feb/2018


classdef BELM < Util
    properties
        maxNumberOfHiddenNeurons  = 1000
        numberOfHiddenNeurons = 0
        activationFunction = 'sig'
        numberOfInputNeurons = []
        inputWeight = []
        biasOfHiddenNeurons = []
        outputWeight = []
        maximumError = 1e-3
    end
    properties (Access = private)
        iActFun
    end
    methods
        function self = BELM(varargin)

            if mod(nargin,2) ~= 0
                exception = MException('BELM:params','Params must be given in pairs');
                throw (exception)
            end
            
            for i=1:2:nargin
                if isprop(self,varargin{i})
                    self.(varargin{i}) = varargin{i+1};
                else
                    exception = MException('BELM:params','Given parameter does not exist');
                    throw (exception)
                end
            end
            
            if isempty(self.numberOfInputNeurons)
                throw(MException('BELM:emptynumberOfInputNeurons','Empty Number of Input Neurons'));
            end
            
            self.seed = self.parseSeed();
            [self.activationFunction,self.iActFun] = self.parseActivationFunction(self.activationFunction);
            
            self.numberOfHiddenNeurons = 0;
            self.inputWeight = [];
            self.biasOfHiddenNeurons = [];
            
        end
        
        
        function [actFun,iact] = parseActivationFunction(~,actFun)
            if isequal(class(actFun),'char')
                switch lower(actFun)
                    case {'sig','sigmoid'}
                        %%%%%%%% Sigmoid
                        actFun = @(tempH) 1 ./ (1 + exp(-tempH));
                        iact = @(tempH) - log( (1./tempH) - 1);
                    case {'sin','sine'}
                        %%%%%%%% Sine
                        actFun = @(tempH) sin(tempH);
                        iact = @(tempH) asin(tempH);
                        %%%%%%%% More activation functions can be added here
                end
            elseif ~isequal(class(actFun),'function_handle')
                throw(MException('BELM:activationFunctionError','Error Activation Function'));
            end
        end
        
        
        function self = train(self, X, Y)
            
            if size(Y,2) ~= 1
                throw(MException('BELM:outputSize','BELM method only supports one dimensional outputs'));
            end
            
            auxTime = toc;
            E = Y;
            pinvX = pinv(X);
            while (self.numberOfHiddenNeurons < self.maxNumberOfHiddenNeurons) && (norm(E,'fro') > self.maximumError)
                self.numberOfHiddenNeurons = self.numberOfHiddenNeurons + 1;
                if mod(self.numberOfHiddenNeurons,2) == 1
                    Wnew = rand(self.seed, self.numberOfInputNeurons, 1)*2-1;
                    Bnew = rand(self.seed, 1, 1);
                    tempH = X*Wnew + repmat(Bnew,size(X,1),1);
                    H2e = self.activationFunction(tempH);
                    clear tempH;
                    betaN = H2e'*E/(H2e'*H2e);
                else
                    H2e = E/betaN;
                    [H2e,PS] = mapminmax(H2e',0.1,0.9); %Function u used in https://raw.githubusercontent.com/ExtremeLearningMachines/Bidirectional-ELM/master/B_ELM.m
                    iH2e = self.iActFun(H2e');
                    Wnew = pinvX*iH2e;  % Wnew = a2n
                    a2nX = X*Wnew;
                    Bnew = self.calculateRMSE(iH2e,a2nX);  % Bnew = b2n
                    H2e = self.activationFunction(a2nX + repmat(Bnew,size(X,1),1));
                    H2e = mapminmax('reverse',H2e,PS);
                    betaN = H2e'*E/(H2e'*H2e);
                end
                E = E - H2e*betaN;
                self.inputWeight = [self.inputWeight, Wnew];
                self.biasOfHiddenNeurons = [self.biasOfHiddenNeurons, Bnew];
                self.outputWeight = [self.outputWeight; betaN];
            end
            self.trainTime = toc - auxTime;
        end
        
        function Yhat = predict(self, X)
            auxTime = toc;
            tempH = X*self.inputWeight + repmat(self.biasOfHiddenNeurons,size(X,1),1);
            H = self.activationFunction(tempH);
            Yhat = H * self.outputWeight;
            self.lastTestTime = toc - auxTime;
        end
    end
end