% RIELM - Robust Incremental Extreme Learning Machine Class
%   Train and Predict a SLFN based on Robust Incremental Extreme Learning Machine
%
%   This code was implemented based on the following paper:
%
%   [1] Shao, Z., Er, M. J., & Wang, N. (2014). 
%       Robust incremental extreme learning machine. 
%       2014 13th International Conference on Control Automation Robotics and Vision, 
%       ICARCV 2014, 2014(December), 607â€“612. 
%       https://doi.org/10.1109/ICARCV.2014.7064373
%       
%   Attributes:
%       Attributes between *.* must be informed.
%       RIELM objects must be created using name-value pair arguments (see the Usage Example).
%
%         *numberOfInputNeurons*:   Number of neurons in the input layer.
%                Accepted Values:   Any positive integer.
%
%          numberOfHiddenNeurons:   Initial number of neurons in the hidden layer
%                Accepted Values:   Any positive integer (defaut = 1).
%
%       maxNumberOfHiddenNeurons:   Maximum number of neurons in the hidden layer
%                Accepted Values:   Any positive integer (defaut = 1000).
%
%                     alphaError:   Maximum permitted % of change between iterations
%                                   of the error (used as stopping criterion)
%                Accepted Values:   Any real number in (0,1]. (default = 1e-3)
%
%             activationFunction:   Activation funcion for hidden layer
%                Accepted Values:   Function handle (see [1]) or one of these strings:
%                                       'sig':     Sigmoid (default)
%                                       'sin':     Sine
%                                       'hardlim': Hard Limit
%                                       'tribas':  Triangular basis function
%                                       'radbas':  Radial basis function
%
%                           seed:   Seed to generate the pseudo-random values.
%                                   This attribute is for reproducible research.
%                Accepted Values:   RandStream object or a integer seed for RandStream.
%
%       Attributes generated by the code:
%
%                    inputWeight:   Weight matrix that connects the input
%                                   layer to the hidden layer
%
%            biasOfHiddenNeurons:   Bias of hidden units
%
%                   outputWeight:   Weight matrix that connects the hidden
%                                   layer to the output layer
%
%   Methods:
%
%          obj = RIELM(varargin):   Creates RIELM objects. varargin should be in
%                                   pairs. Look attributes
%
%           obj = obj.train(X,Y):   Method for training. X is the input of size N x n,
%                                   where N is (# of samples) and n is the (# of features).
%                                   Y is the output of size N x m, where m is (# of multiple outputs)
%
%          Yhat = obj.predict(X):   Predicts the output for X.
%
%   Usage Example:
%
%       load iris_dataset.mat
%       X    = irisInputs';
%       Y    = irisTargets';
%       rielm  = RIELM('numberOfInputNeurons', 4, 'numberOfHiddenNeurons',100);
%       rielm  = rielm.train(X, Y);
%       Yhat = rielm.predict(X)
%
%   License:
%
%   Permission to use, copy, or modify this software and its documentation
%   for educational and research purposes only and without fee is here
%   granted, provided that this copyright notice and the original authors'
%   names appear on all copies and supporting documentation. This program
%   shall not be used, rewritten, or adapted as the basis of a commercial
%   software or hardware product without first obtaining permission of the
%   authors. The authors make no representations about the suitability of
%   this software for any purpose. It is provided "as is" without express
%   or implied warranty.
%
%       Federal University of Espirito Santo (UFES), Brazil
%       Computers and Neural Systems Lab. (LabCISNE)
%       Authors:    B. L. S. Silva, F. K. Inaba, D. L. Cosmo
%       email:      labcisne@gmail.com
%       website:    github.com/labcisne/ELMToolbox
%       date:       Feb/2018


classdef RIELM < ELM
    properties
        maxNumberOfHiddenNeurons  = 1000
        pseudoInv
        numberOfNeuronsByStep = 10
        alphaError = 1e-3;
    end
    properties (Access = private)
        H;
    end
    
    methods
        function self = RIELM(varargin)
            self = self@ELM('numberOfHiddenNeurons',1,varargin{:});
        end
        
        function pi = pseudoinverse(~,h)
            if size(h,1)>=size(h,2)
                pi = pinv(h' * h) * h';
            else
                pi = h' * pinv(h * h');
            end
        end
        
        function self = train(self, X, Y)
            auxTime = toc;
            %Train with initial number of hidden neurons
            tempH = X*self.inputWeight + repmat(self.biasOfHiddenNeurons,size(X,1),1);
            self.H = self.activationFunction(tempH);
            
            self.pseudoInv = self.pseudoinverse(self.H);
            self.outputWeight = self.pseudoInv * Y;
            
            % This equals diag(self.H * self.pseudoInv)
            % which are the HAT values used in the LOO equation (only hii values)
            HATdiag = sum(self.H .* self.pseudoInv',2);
            
            %             hat = diag(HATdiag);
            
            T1hat = self.H*self.outputWeight;
            
            ELoo = T1hat - Y./HATdiag;
            ELoo = mean(ELoo(:).^2);
            LOOmin = ELoo;
            eta = self.alphaError*LOOmin;
            Lopt = self.numberOfHiddenNeurons;
            betaOpt = self.outputWeight;
            
            for k=self.numberOfHiddenNeurons+1:self.maxNumberOfHiddenNeurons
                
                Wnew = rand(self.seed, self.numberOfInputNeurons, 1)*2-1;
                Bnew = rand(self.seed, 1, 1);
                self.inputWeight = [self.inputWeight, Wnew];
                self.biasOfHiddenNeurons = [self.biasOfHiddenNeurons, Bnew];
                tempH = X*Wnew + repmat(Bnew,size(X,1),1);
                deltaH = self.activationFunction(tempH);
                    
                Dk = eye(size(self.H,1)) - self.H*self.pseudoInv;
                Dk = self.pseudoinverse( Dk*deltaH);
                Uk = self.pseudoInv*(eye(size(Dk,2)) - deltaH*Dk);
                self.pseudoInv = [Uk; Dk];
                self.H = [self.H, deltaH];
                
                HATdiag = sum(self.H .* self.pseudoInv',2); 
                
                self.outputWeight = self.pseudoInv * Y;
                T1hat = self.H*self.outputWeight;
                ELoo = T1hat - Y./HATdiag;
                ELoo = mean(ELoo(:).^2);
                
                if (ELoo < LOOmin)
                    LOOmin = ELoo;
                    eta = self.alphaError*LOOmin;
                    Lopt = k; %k+1
                    betaOpt = self.outputWeight;
                elseif ((ELoo - LOOmin) > eta)
                    break
                end
            end
            
%             Lopt = self.numberOfHiddenNeurons + Lopt;
            self.inputWeight = self.inputWeight(:,1:Lopt);
            self.biasOfHiddenNeurons = self.biasOfHiddenNeurons(1:Lopt);
            self.outputWeight = betaOpt;
            self.numberOfHiddenNeurons = Lopt;
            
            self.trainTime = toc - auxTime;
        end
    end
end
