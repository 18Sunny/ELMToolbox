% ORELM - Outlier Robust Extreme Learning Machine Class
%   Train and Predict a SLFN based on Outlier Robust Extreme Learning Machine
%
%   This code was implemented based on the following paper:
%
%   [1] Kai Zhang, Minxia Luo, Outlier-robust extreme learning machine for 
%       regression problems, Neurocomputing, Volume 151, Part 3, 2015, 
%       Pages 1519-1527, ISSN 0925-2312,
%       https://doi.org/10.1016/j.neucom.2014.09.022.
%       (http://www.sciencedirect.com/science/article/pii/S0925231214012053)
%
%   Attributes: 
%       Attributes between *.* must be informed.
%       OR-ELM objects must be created using name-value pairs (see the Usage Example).
%
%         *NumberofInputNeurons*:   Number of neurons in the input layer
%                Accepted Values:   Any positive integer.
%
%          NumberofHiddenNeurons:   Number of neurons in the hidden layer
%                Accepted Values:   Any positive integer (defaut = 1000).
%
%       Regularization_parameter:   Regularization Parameter (defaut = 1000)
%                Accepted Values:   Any positive real number.
%
%                        maxIter:   Max. number of iteration on ALM (defaut = 20)
%
%             ActivationFunction:   Activation funcion for hidden layer   
%                Accepted Values:   Function handle (see [1]) or one of these strings:
%                                       'sig':     Sigmoid (default)
%                                       'sin':     Sine
%                                       'hardlim': Hard Limit
%                                       'tribas':  Triangular basis function
%                                       'radbas':  Radial basis function
%
%                           Seed:   Seed to generate the pseudo-random values.
%                                   This attribute is for reproducible research.
%                Accepted Values:   RandStream object or a integer seed for RandStream.
%
%       Attributes generated by the code:
%
%       InputWeight:             Weight matrix that connects the input 
%                                layer to the hidden layer
%
%       BiasofHiddenNeurons:     Bias of hidden units
%
%       OutputWeight:            Weight matrix that connects the hidden
%                                layer to the output layer
%
%   Methods:
%
%       obj = ORELM(varargin):        Creates ORELM objects. varargin should be in
%                                     pairs. Look attributes.
%
%       obj = obj.train(X,Y):         Method for training. X is the input of size N x n,
%                                     where N is (# of samples) and n is the (# of features).
%                                     Y is the output of size N x m, where m is (# of multiple outputs)
%                            
%       Yhat = obj.predict(X):        Predicts the output for X.
%
%   Usage Example:
%
%       load iris_dataset.mat
%       X      = irisInputs';
%       Y      = irisTargets';
%       orelm  = ORELM('NumberofInputNeurons', 4);
%       orelm  = orelm.train(X, Y);
%       Yhat   = orelm.predict(X);

%   License:
%
%   Permission to use, copy, or modify this software and its documentation
%   for educational and research purposes only and without fee is here
%   granted, provided that this copyright notice and the original authors'
%   names appear on all copies and supporting documentation. This program
%   shall not be used, rewritten, or adapted as the basis of a commercial
%   software or hardware product without first obtaining permission of the
%   authors. The authors make no representations about the suitability of
%   this software for any purpose. It is provided "as is" without express
%   or implied warranty.
%
%       Federal University of Espirito Santo (UFES), Brazil
%       Computers and Neural Systems Lab. (LabCISNE)
%       Authors:    F. K. Inaba, B. L. S. Silva, D. L. Cosmo 
%       email:      labcisne@gmail.com
%       website:    github.com/labcisne/ELMToolbox
%       date:       Jan/2018

classdef ORELM
    properties
        maxIter = 20
        NumberofHiddenNeurons = 1000
        Regularization_parameter = 1000
        ActivationFunction = 'sig'
        NumberofInputNeurons = []
        InputWeight = []
        BiasofHiddenNeurons = []
        OutputWeight = []
        Seed = []
    end
    methods
        function obj = ORELM(varargin)
            for i = 1:2:nargin
                obj.(varargin{i}) = varargin{i+1};
            end
            if isnumeric(obj.Seed) && ~isempty(obj.Seed)
                obj.Seed = RandStream('mt19937ar','Seed', obj.Seed);
            elseif ~isa(obj.Seed, 'RandStream')
                obj.Seed = RandStream.getGlobalStream();
            end
            if isempty(obj.NumberofInputNeurons)
                throw(MException('EmptyNumberOfInputNeurons','Empty Number of Input Neurons'));
            end
            obj.InputWeight = rand(obj.Seed, obj.NumberofInputNeurons, obj.NumberofHiddenNeurons)*2-1;
            obj.BiasofHiddenNeurons = rand(obj.Seed, 1, obj.NumberofHiddenNeurons);
            
            if ~isa(obj.ActivationFunction,'function_handle') && ischar(obj.ActivationFunction)
                switch lower(obj.ActivationFunction)
                    case {'sig','sigmoid'}
                        %%%%%%%% Sigmoid
                        obj.ActivationFunction = @(tempH) 1 ./ (1 + exp(-tempH));
                    case {'sin','sine'}
                        %%%%%%%% Sine
                        obj.ActivationFunction = @(tempH) sin(tempH);
                    case {'hardlim'}
                        %%%%%%%% Hard Limit
                        obj.ActivationFunction = @(tempH) double(hardlim(tempH));
                    case {'tribas'}
                        %%%%%%%% Triangular basis function
                        obj.ActivationFunction = @(tempH) tribas(tempH);
                    case {'radbas'}
                        %%%%%%%% Radial basis function
                        obj.ActivationFunction = @(tempH) radbas(tempH);
                        %%%%%%%% More activation functions can be added here
                end
            else
                throw(MException('ActivationFunctionError','Error Activation Function'));
            end
        end       
        
        function self = train(self, X, Y)
            tempH = X*self.InputWeight + repmat(self.BiasofHiddenNeurons,size(X,1),1);
            clear X;
            H = self.ActivationFunction(tempH);
            
            %--------------------------------------------------------------
            % ALM Algorithm for finding beta
            %--------------------------------------------------------------
            [m,n]  = size(H) ;
            kappa  = 1/self.Regularization_parameter;
            nIter  = 0 ;
            mu     = 2*m/norm(Y,1);
            lambda = zeros(m,1);
            e      = zeros(m,1);
            converged_main = 0;
            muInv  = 1/mu ;
            if n<m
                Proj_M = pinv(H'*H+2*kappa*muInv*eye(n))*H';
            else
                Proj_M = H'*pinv(H*H'+2*kappa*muInv*eye(m));
            end
            
            while ~converged_main
                lambdaScaled = muInv*lambda ;
                nIter  = nIter + 1 ;
                beta   = Proj_M*(Y-e+lambdaScaled );   %-------------(1)
                temp   = Y + lambdaScaled - H*beta;
                e = sign(temp).*max(abs(temp)-muInv,0);%-------------(2)
                lambda = lambda + mu*(Y - H*beta - e); %-------------(3)
                if nIter >= self.maxIter
                    converged_main = 1 ;
                end
            end
            self.OutputWeight = beta;
            %--------------------------------------------------------------
        end
        function Yhat = predict(self, X)
            tempH = X*self.InputWeight + repmat(self.BiasofHiddenNeurons,size(X,1),1);
            clear X;
            H = self.ActivationFunction(tempH);
            Yhat = H * self.OutputWeight;
        end
    end
end