%   DSNELM - Deep Stacked Network constructed using SLFN modules trained with the R-ELM algorithm
%   Train and Predict a DSN based on Extreme Learning Machine
%
%   This code was implemented based on the following papers:
%
%   [1] Deng, L.,  D Yu. (2011).
%       Deep convex network: A scalable architecture for deep learning.
%       In Interspeech. International Speech Communication Association.
%       Retrieved from https://scholar.google.co.uk/scholar?q=deep+learning&hl=en&as_sdt=0,5#770
%       (https://www.microsoft.com/en-us/research/publication/deep-convex-network-a-scalable-architecture-for-speech-pattern-classification/)
%
%   [2] Deng, L. (2014).
%       A Tutorial Survey of Architectures, Algorithms, and Applications for Deep Learning.
%       APSIPA Transactions on Signal and Information Processing.
%       (https://www.microsoft.com/en-us/research/publication/a-tutorial-survey-of-architectures-algorithms-and-applications-for-deep-learning/)
%
%   Attributes:
%       Attributes between *.* must be informed.
%       DSNELM objects must be created using name-value pair arguments (see the Usage Example).
%
%         *numberOfInputNeurons*:   Number of neurons in the input layer.
%                Accepted Values:   Any positive integer.
%
%          numberOfHiddenNeurons:   Number of neurons in the hidden layer
%                Accepted Values:   Any positive integer (defaut = 1000).
%
%        regularizationParameter:   Regularization Parameter (defaut = 1000)
%                Accepted Values:   Any positive real number.
%
%             maxNumberOfModules:   Number of modules of the network
%                Accepted Values:   Any positive integer number. (default = 100)
%
%             activationFunction:   Activation funcion for hidden layer
%                Accepted Values:   Function handle (see [1]) or one of these strings:
%                                       'sig':     Sigmoid (default)
%                                       'sin':     Sine
%                                       'hardlim': Hard Limit
%                                       'tribas':  Triangular basis function
%                                       'radbas':  Radial basis function
%
%                           seed:   Seed to generate the pseudo-random values.
%                                   This attribute is for reproducible research.
%                Accepted Values:   RandStream object or a integer seed for RandStream.
%
%       Attributes generated by the code:
%
%                    inputWeight:   Weight matrix that connects the input
%                                   layer to the hidden layer
%
%            biasOfHiddenNeurons:   Bias of hidden units
%
%                   outputWeight:   Weight matrix that connects the hidden
%                                   layer to the output layer

%                 stackedModules:   List of module objects of the network
%
%
%   Methods:
%
%         obj = DSNELM(varargin):   Creates DSNELM objects. varargin should be in
%                                   pairs. Look attributes
%
%           obj = obj.train(X,Y):   Method for training. X is the input of size N x n,
%                                   where N is (# of samples) and n is the (# of features).
%                                   Y is the output of size N x m, where m is (# of multiple outputs)
%
%          Yhat = obj.predict(X):   Predicts the output for X.
%
%   Usage Example:
%
%       load iris_dataset.mat
%       X    = irisInputs';
%       Y    = irisTargets';
%       dsnelm  = DSNELM('numberOfInputNeurons', 4, 'numberOfHiddenNeurons', 100);
%       dsnelm  = dsnelm.train(X, Y);
%       Yhat = dsnelm.predict(X)

%   License:
%
%   Permission to use, copy, or modify this software and its documentation
%   for educational and research purposes only and without fee is here
%   granted, provided that this copyright notice and the original authors'
%   names appear on all copies and supporting documentation. This program
%   shall not be used, rewritten, or adapted as the basis of a commercial
%   software or hardware product without first obtaining permission of the
%   authors. The authors make no representations about the suitability of
%   this software for any purpose. It is provided "as is" without express
%   or implied warranty.
%
%       Federal University of Espirito Santo (UFES), Brazil
%       Computers and Neural Systems Lab. (LabCISNE)
%       Authors:    F. K. Inaba, B. L. S. Silva, D. L. Cosmo
%       email:      labcisne@gmail.com
%       website:    github.com/labcisne/ELMToolbox
%       date:       Jan/2018

classdef DSNELM
    properties (SetAccess = protected, GetAccess = public)
        stackedModules = []
        maxNumberOfModules = 100
        activationFunction = 'sig'
        numberOfHiddenNeurons = 1000
        numberOfInputNeurons = []
        regularizationParameter = 1000
        seed = []
    end
    
    methods
        
        function obj = DSNELM(varargin)
            
            if mod(nargin,2) ~= 0
                exception = MException('DSNELM:ParameterError','Params must be given in pairs');
                throw (exception)
            end
            
            for i=1:2:nargin
                if isprop(obj,varargin{i})
                    obj.(varargin{i}) = varargin{i+1};
                else
                    exception = MException('DSNELM:ParameterError','Given parameter does not exist');
                    throw (exception)
                end
            end
            
            if isnumeric(obj.seed) && ~isempty(obj.seed)
                obj.seed = RandStream('mt19937ar','Seed', obj.seed);
            elseif ~isa(obj.seed, 'RandStream')
                obj.seed = RandStream.getGlobalStream();
            end
            
            if isequal(class(obj.activationFunction),'char')
                switch lower(obj.activationFunction)
                    case {'sig','sigmoid'}
                        %%%%%%%% Sigmoid
                        obj.activationFunction = @(tempH) 1 ./ (1 + exp(-tempH));
                    case {'sin','sine'}
                        %%%%%%%% Sine
                        obj.activationFunction = @(tempH) sin(tempH);
                    case {'hardlim'}
                        %%%%%%%% Hard Limit
                        obj.activationFunction = @(tempH) double(hardlim(tempH));
                    case {'tribas'}
                        %%%%%%%% Triangular basis function
                        obj.activationFunction = @(tempH) tribas(tempH);
                    case {'radbas'}
                        %%%%%%%% Radial basis function
                        obj.activationFunction = @(tempH) radbas(tempH);
                        %%%%%%%% More activation functions can be added here
                end
            elseif ~isequal(class(obj.activationFunction),'function_handle')
                exception = MException('DSNELM:activationFunctionError','Hidden activation function not supported');
                throw (exception)
            end
            
            if isempty(obj.numberOfInputNeurons)
                throw(MException('RELM:emptyNumberOfInputNeurons','Empty Number of Input Neurons'));
            end
            
            obj.stackedModules = [];
            
        end
        
        function self = train(self,inputData,outputData)
            if (size(inputData,2) ~= self.numberOfInputNeurons)
                exception = MException('DSNELM:wrongNumberOfInputNeurons','Wrong input dimension!');
                throw(exception);
            end
            
            lastLayerOutput = [];
            while length(self.stackedModules) < self.maxNumberOfModules
                
                params = cell(1,2*5);
                params(1:2) = {'numberOfInputNeurons',size(inputData,2)+size(lastLayerOutput,2)};
                params(3:4) = {'numberOfHiddenNeurons',self.numberOfHiddenNeurons};
                params(5:6) = {'regularizationParameter',self.regularizationParameter};
                params(7:8) = {'activationFunction',self.activationFunction};
                params(9:10) = {'seed',self.seed};
                
                newModule = RELM(params{:});
                
                aux = zeros(size(inputData,1),size(inputData,2)+size(lastLayerOutput,2));
                aux(:,1:size(inputData,2)) = inputData;
                if ~isempty(lastLayerOutput)
                    aux(:,size(inputData,2)+1:end) = lastLayerOutput;
                end
                inputData = aux;
                
                newModule = newModule.train(inputData,outputData);
                lastLayerOutput = newModule.predict(inputData);
                self.stackedModules = [self.stackedModules, newModule];
            end
        end
        
        function lastLayerOutput = predict(self, inputData)
            if (size(inputData,2) ~= self.numberOfInputNeurons)
                exception = MException('DSNELM:wrongNumberOfInputNeurons','Wrong input dimension!');
                throw(exception);
            end
            
            lastLayerOutput = [];
            for i=1:length(self.stackedModules)
                aux = zeros(size(inputData,1),size(inputData,2)+size(lastLayerOutput,2));
                aux(:,1:size(inputData,2)) = inputData;
                aux(:,size(inputData,2)+1:end) = lastLayerOutput;
                inputData = aux;
                
                lastLayerOutput = self.stackedModules(i).predict(inputData);
            end
        end
        
        % Function used to predict the outputs in every module
        %         function predCell = predictModules(self,inputData)
        %
        %             if (size(inputData,2) ~= self.numberOfInputNeurons)
        %                 exception = MException('DSNELM:wrongNumberOfInputNeurons','Wrong input dimension!');
        %                 throw(exception);
        %             end
        %
        %             lastLayerOutput = [];
        %             for i=1:length(self.stackedModules)
        %                 aux = zeros(size(inputData,1),size(inputData,2)+size(lastLayerOutput,2));
        %                 aux(:,1:size(inputData,2)) = inputData;
        %                 aux(:,size(inputData,2)+1:end) = lastLayerOutput;
        %                 inputData = aux;
        %
        %                 lastLayerOutput = self.stackedModules(i).predict(inputData);
        %                 predCell{i} = lastLayerOutput;
        %             end
        %
        %         end
        
    end
end
