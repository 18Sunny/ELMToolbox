%   AESELM - Autoencoder Stacked Extreme Learning Machine Class
%   Train and Predict a Autoencoder Stacked network based on Extreme Learning Machine
%
%   This code was implemented based on the following paper:
%
%   [1] Zhou, H., Huang, G.-B., Lin, Z., Wang, H., & Soh, Y. C. (2014).
%       Stacked Extreme Learning Machines.
%       IEEE Transactions on Cybernetics, PP(99), 1.
%       https://doi.org/10.1109/TCYB.2014.2363492
%       (http://ieeexplore.ieee.org/document/6937189/)
%
%
%   Attributes:
%       Attributes between *.* must be informed.
%       AESELM objects must be created using name-value pair arguments (see the Usage Example).
%
%         *numberOfInputNeurons*:   Number of neurons in the input layer.
%                Accepted Values:   Any positive integer.
%
%          numberOfHiddenNeurons:   Number of neurons in the hidden layer
%                Accepted Values:   Any positive integer (defaut = 1000).
%
%               reducedDimension:   Number of nodes after the PCA dimensionality reduction. See [1].
%                Accepted Values:   Any integer between 1 and numberOfInputNeurons-1. (default 100)
%
%        regularizationParameter:   Regularization Parameter (defaut = 1000)
%                Accepted Values:   Any positive real number.
%
%             maxNumberOfModules:   Number of modules of the network
%                Accepted Values:   Any positive integer number. (default = 100)
%
%             activationFunction:   Activation funcion for hidden layer
%                Accepted Values:   Function handle (see [1]) or one of these strings:
%                                       'sig':     Sigmoid (default)
%                                       'sin':     Sine
%                                       'hardlim': Hard Limit
%                                       'tribas':  Triangular basis function
%                                       'radbas':  Radial basis function
%
%                           seed:   Seed to generate the pseudo-random values.
%                                   This attribute is for reproducible research.
%                Accepted Values:   RandStream object or a integer seed for RandStream.
%
%       Attributes generated by the code:
%
%                    inputWeight:   Weight matrix that connects the input
%                                   layer to the hidden layer
%
%            biasOfHiddenNeurons:   Bias of hidden units
%
%                   outputWeight:   Weight matrix that connects the hidden
%                                   layer to the output layer

%                 stackedModules:   List of module objects of the network
%
%
%   Methods:
%
%       obj = AESELM(varargin):        Creates RELM objects. varargin should be in
%                                    pairs. Look attributes
%
%       obj = obj.train(X,Y):        Method for training. X is the input of size N x n,
%                                    where N is (# of samples) and n is the (# of features).
%                                    Y is the output of size N x m, where m is (# of multiple outputs)
%
%       Yhat = obj.predict(X):       Predicts the output for X.
%
%   Usage Example:
%
%       load iris_dataset.mat
%       X    = irisInputs';
%       Y    = irisTargets';
%       aeselm  = AESELM('numberOfInputNeurons', 4, 'numberOfHiddenNeurons', 100);
%       aeselm  = aeselm.train(X, Y);
%       Yhat = aeselm.predict(X)

%   License:
%
%   Permission to use, copy, or modify this software and its documentation
%   for educational and research purposes only and without fee is here
%   granted, provided that this copyright notice and the original authors'
%   names appear on all copies and supporting documentation. This program
%   shall not be used, rewritten, or adapted as the basis of a commercial
%   software or hardware product without first obtaining permission of the
%   authors. The authors make no representations about the suitability of
%   this software for any purpose. It is provided "as is" without express
%   or implied warranty.
%
%       Federal University of Espirito Santo (UFES), Brazil
%       Computers and Neural Systems Lab. (LabCISNE)
%       Authors:    B. L. S. Silva, F. K. Inaba, D. L. Cosmo
%       email:      labcisne@gmail.com
%       website:    github.com/labcisne/ELMToolbox
%       date:       Jan/2018


classdef AESELM < SELM
    methods
        
        function obj = AESELM(varargin)
            obj = obj@SELM(varargin{:});
        end
        
        function self = train(self,inputData,outputData)
            if (size(inputData,2) ~= self.numberOfInputNeurons)
                exception = MException('AESELM:wrongNumberOfInputNeurons','Wrong input dimension!');
                throw(exception);
            end
            
            aux = toc;
            lastHiddenOutput = [];
            while length(self.stackedModules) < self.maxNumberOfModules
                
                params = cell(1,2*8);
                params(1:2) = {'numberOfInputNeurons',self.numberOfInputNeurons};
                params(3:4) = {'numberOfHiddenNeurons',self.numberOfHiddenNeurons};
                params(5:6) = {'regularizationParameter',self.regularizationCoefficient};
                params(7:8) = {'activationFunction',self.activationFunction};
                params(9:10) = {'reducedDimension',self.reducedDimension};
                params(11:12) = {'isFirstLayer',isempty(self.stackedModules)};
                params(13:14) = {'isLastLayer',isequal(length(self.stackedModules),self.maxNumberOfModules-1)};
                params(15:16) = {'seed',self.seed};
                
                
                newModule = AESELMModule(params{:});
                [newModule, lastHiddenOutput] = newModule.train(inputData,outputData,lastHiddenOutput);
                self.stackedModules = [self.stackedModules, newModule];
            end
            self.trainTime = toc - aux;
        end
        
    end
end